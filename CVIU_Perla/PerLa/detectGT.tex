\subsection{Goals of our project}
Here a description of our platform for generating ground truth (hand-labeling, crowdsourcing), testing algorithms performance and validating results.\\ 
\subsection{Hand-Labeled GT on Underwater Videos}
\subsubsection{Web-based Platform for Hand-Labeling Ground Truth}
One of the most widely used tool for generating ground truth on videosequences is ViPer. More specifically, it allows frame-by-frame markup of video
stored in a specific XML format. \\
Improvements with respect to the ViPer\\
Semantic Information on detected blobs
\begin{itemize}
%\item Description of our tool for hand-labeling GT.\\
\item Functionalities of the web-annotation tool:
\begin{enumerate}
\item Description on how to use it:
\begin{enumerate}
\item new blob creation;
\item blob deletion
\item contour extraction (both manually and automatically)
\item semantic label association
\end{enumerate}
\item Interoperability with existing tools
\item XML Schema for GT
\end{enumerate}
\item Combing GT of different users
\begin{enumerate}
\item Overlap Score: PASCAL VOC
\item Euclidean Distance Score
\end{enumerate}
\item Ground Truth Quality Ranking. Annotation Scoring Functions
\begin{enumerate} 
\item Control Points
\item Annotation size
\item Edge Detection
\item Bayesian Matting
\item Active/Statistical Shape Models
\end{enumerate}
\end{itemize}
\subsubsection{Content of the collected GT}
\subsection{Performance Evaluation (PE)}
VIVID evaluation website: not web-platform
\begin{itemize}
\item PE of Detection Algorithms on :
\begin{enumerate}
\item Hand-Labeled GT Data;
\item Crowsourced GT Data
\end{enumerate}
\item PE of Tracking Algorithms
\end{itemize}
